{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from lib import get_stop_words, get_text, get_lem_words\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat, chain\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "NUM_PROCESSES = 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# project gutenberg text has a lot of extra stuff at the beginning and end\n",
    "def get_text_no_gutenberg(raw_text):\n",
    "    return re.split('^\\*\\*\\*(.*)\\*\\*\\*$', raw_text, flags=re.MULTILINE)[2]\n",
    "    \n",
    "def remove_single_letter(text):\n",
    "    text = re.sub(r'\\b\\w\\b', ' ', text)\n",
    "    return text\n",
    "\n",
    "# split the raw text into chapters\n",
    "def split_chapters(text, roman_numeral=True):\n",
    "    if roman_numeral:\n",
    "        return re.split(r'CHAPTER [IVXLCDM]+', text, flags=re.IGNORECASE)\n",
    "    else:\n",
    "        return re.split(r'CHAPTER \\d+', text, flags=re.IGNORECASE)\n",
    "\n",
    "# sometimes the contents lists chapters that are in table of contents. here we just remove chapters that are too short.\n",
    "# here we also delete the first chapter, which is just the table of contents and preface\n",
    "def chapter_longer_than(raw_chapters, n=150):\n",
    "    return list(filter(lambda c: len(c) > n, raw_chapters))\n",
    "\n",
    "# split remove stopwords\n",
    "def remove_stopwords(text, stopwords):\n",
    "    return re.sub(r'\\b(' + '|'.join(stopwords) + r')\\b', '', text)\n",
    "\n",
    "# theres a ton of whitespace that we dont want\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def replace_common_dots(text):\n",
    "    text =  re.sub(r'dr\\.', 'dr ', text)\n",
    "    text =  re.sub(r'mr\\. ', 'mr ', text)\n",
    "    return text\n",
    "\n",
    "def compress_abbr(text):\n",
    "    pattern = r'\\b(?:[a-zA-Z]\\.){2,}\\b.*?'\n",
    "    matches = re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
    "    abbr = set(map(lambda m: m.group(), matches))\n",
    "    for a in abbr:\n",
    "        text = text.replace(a, a.replace('.', ''))\n",
    "    return text\n",
    "\n",
    "def get_lem(fn):\n",
    "    lem = {}\n",
    "    lem_regex = []\n",
    "    with open(fn, 'r', encoding='utf-8-sig') as f:\n",
    "        for i in f:\n",
    "            text = [j for j in i.lower().split()]\n",
    "            if text[0] not in lem:\n",
    "                lem[text[0]] = [text[1]]\n",
    "            else:\n",
    "                lem[text[0]].append(text[1])\n",
    "    for word, lemmas in lem.items():\n",
    "        lem_regex.append([word, fr'\\b({\"|\".join(lemmas)})\\b'])\n",
    "    return lem_regex\n",
    "def remove_punctuation(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = re.sub(r'[^\\w\\s]', lambda m: \".\" if m.group(0) == \".\" else \" \", text[i])\n",
    "    return text\n",
    "\n",
    "def split_sentences(text):\n",
    "    return re.split(r\"[.]\", text)\n",
    "\n",
    "def lemmatization(text, lem):\n",
    "    for lemma, lem_regex in lem:\n",
    "        text = re.sub(lem_regex, lemma, text)\n",
    "    return text\n",
    "\n",
    "def trim(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].strip()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "books = [\n",
    "    {\n",
    "        'fn': 'a-study-in-scarlet.txt',\n",
    "        'roman_numeral': True\n",
    "    },\n",
    "    {\n",
    "        'fn': 'arsene-lupin-vs-sherlock-holmes.txt',\n",
    "        'roman_numeral': True\n",
    "    },\n",
    "    {\n",
    "        'fn': 'the-hound-of-the-baskervilles.txt',\n",
    "        'roman_numeral': False\n",
    "    },\n",
    "    {\n",
    "        'fn':  'the-mystery-of-cloomber.txt',\n",
    "        'roman_numeral': True\n",
    "    },\n",
    "    {\n",
    "        'fn': 'the-sign-of-four.txt',\n",
    "        'roman_numeral': True\n",
    "    },\n",
    "]\n",
    "basedir = '../books/'\n",
    "stop_words = get_stop_words('stopwords.txt')\n",
    "lem = get_lem('lemmatization-en.txt')\n",
    "book_i = 2\n",
    "book_fn = books[book_i]['fn']\n",
    "book_rom_num = books[book_i]['roman_numeral']\n",
    "\n",
    "def clean_all_text(book_fn, base_dir, stop_words, lem, book_rom_num):\n",
    "\n",
    "    # get the raw text and make it all lower case\n",
    "    raw_text = get_text(book_fn, base_dir)\n",
    "    raw_text = raw_text.lower()\n",
    "    raw_text = get_text_no_gutenberg(raw_text)\n",
    "    raw_text = remove_stopwords(raw_text, stop_words)\n",
    "    raw_text = remove_extra_spaces(raw_text)\n",
    "    raw_text = replace_common_dots(raw_text)\n",
    "    raw_text = compress_abbr(raw_text)\n",
    "\n",
    "    raw_chapters = split_chapters(raw_text, book_rom_num)\n",
    "    raw_chapters = chapter_longer_than(raw_chapters)[1:]\n",
    "    raw_chapters = list(map(lambda c: remove_stopwords(c, stop_words), raw_chapters))\n",
    "    raw_chapters = list(map(remove_extra_spaces, raw_chapters))\n",
    "    raw_chapters  = remove_punctuation(raw_chapters)\n",
    "    raw_chapters = list(map(remove_extra_spaces, raw_chapters))\n",
    "\n",
    "    # lemmatization by chapter so we can parallelize it\n",
    "    with Pool(NUM_PROCESSES) as pool:\n",
    "        raw_chapters = pool.starmap(lemmatization, zip(raw_chapters, repeat(lem)))\n",
    "\n",
    "    def sentence_helper(text):\n",
    "        s = split_sentences(text)\n",
    "        s = trim(s)\n",
    "        s = list(filter(lambda x: x != '', s))\n",
    "        return s\n",
    "    raw_chapter_sentences = list(map(sentence_helper, raw_chapters))\n",
    "\n",
    "    raw_text = '. '.join(chain(*raw_chapter_sentences))\n",
    "    raw_text = remove_extra_spaces(raw_text)\n",
    "\n",
    "    # here we are seperating each sentence by new line and appending the chapter and sentence number to the beginning of each sentence\n",
    "    marked_text = ''\n",
    "    for i in range(len(raw_chapter_sentences)):\n",
    "        for j in range(len(raw_chapter_sentences[i])):\n",
    "            marked_text += f'((({i};;;{j}))) {raw_chapter_sentences[i][j]}\\n'\n",
    "    return raw_text, raw_chapters, raw_chapter_sentences, marked_text\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# save the cleaned text\n",
    "def save_cleaned_text(book_fn, base_dir, raw_text, raw_chapters, raw_chapter_sentences, marked_text):\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(raw_text)\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned_chapters.pkl'), 'wb') as f:\n",
    "        pickle.dump(raw_chapters, f)\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned_chapter_sentences.pkl'), 'wb') as f:\n",
    "        pickle.dump(raw_chapter_sentences, f)\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned_marked.txt'), 'w', encoding='utf-8') as f:\n",
    "        f.write(marked_text)\n",
    "\n",
    "# read in the cleaned text\n",
    "def read_cleaned_text(book_fn, base_dir):\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned.txt'), 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned_chapters.pkl'), 'rb') as f:\n",
    "        raw_chapters = pickle.load(f)\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned_chapter_sentences.pkl'), 'rb') as f:\n",
    "        raw_chapter_sentences = pickle.load(f)\n",
    "    with open(os.path.join(base_dir, book_fn + '_cleaned_marked.txt'), 'r', encoding='utf-8') as f:\n",
    "        marked_text = f.read()\n",
    "\n",
    "    return raw_text, raw_chapters, raw_chapter_sentences, marked_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning a-study-in-scarlet.txt\n",
      "Saving a-study-in-scarlet.txt\n",
      "Cleaning arsene-lupin-vs-sherlock-holmes.txt\n",
      "Saving arsene-lupin-vs-sherlock-holmes.txt\n",
      "Cleaning the-hound-of-the-baskervilles.txt\n",
      "Saving the-hound-of-the-baskervilles.txt\n",
      "Cleaning the-mystery-of-cloomber.txt\n",
      "Saving the-mystery-of-cloomber.txt\n",
      "Cleaning the-sign-of-four.txt\n",
      "Saving the-sign-of-four.txt\n"
     ]
    }
   ],
   "source": [
    "# go through the process of cleaning the text then write out the cleaned text to files\n",
    "basedir = '../books/'\n",
    "stop_words = get_stop_words('stopwords.txt')\n",
    "lem = get_lem('lemmatization-en.txt')\n",
    "\n",
    "for book_i in range(len(books)):\n",
    "    book_fn = books[book_i]['fn']\n",
    "    book_rom_num = books[book_i]['roman_numeral']\n",
    "    print(f'Cleaning {book_fn}')\n",
    "    raw_text, raw_chapters, raw_chapter_sentences, marked_text = clean_all_text(book_fn, basedir, stop_words, lem, book_rom_num)\n",
    "    print(f'Saving {book_fn}')\n",
    "    save_cleaned_text(book_fn, 'clean-data', raw_text, raw_chapters, raw_chapter_sentences, marked_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# example of reading in cleaned text\n",
    "book_i = 2\n",
    "book_fn = books[book_i]['fn']\n",
    "raw_text, raw_chapters, raw_chapter_sentences, marked_text = read_cleaned_text(book_fn, 'clean-data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'mr sherlock holmes mr sherlock holmes usually late morning save infrequent occasion night seat breakfast table. stand hearth rug pick stick visitor leave night. fine thick piece wood bulbous head sort know penang lawyer. head broad silver band nearly inch. james mortimer mrcs. friend cch. engrave date 1884. stick old fashion family practitioner carry dignify solid reassure. watson holmes sit give sign occupation. know believe eye head. polish silver plate coffee pot say. tell watson visitor stick unfortunate miss notion errand accidental souvenir importance. let hear reconstruct man examination. think say follow far method companion dr mortimer successful elderly medical man esteem know mark appreciation. good say holmes. excellent think probability favour country practitioner great deal visit foot. stick originally handsome knock hardly imagine town practitioner carry. thick iron ferrule wear evident great walk. perfectly sound say holmes. friend cch. guess hunt local hunt member poss'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'. mr sherlock holmes mr sherlock holmes usually late morning save infrequent occasion night seat breakfast table. stand hearth rug pick stick visitor leave night . fine thick piece wood bulbous head sort know penang lawyer. head broad silver band nearly inch . james mortimer mrcs. friend cch. engrave date 1884. stick old fashion family practitioner carry dignify solid reassure. watson holmes sit give sign occupation. know believe eye head. polish silver plate coffee pot say . tell watson visitor stick unfortunate miss notion errand accidental souvenir importance. let hear reconstruct man examination . think say follow far method companion dr mortimer successful elderly medical man esteem know mark appreciation. good say holmes. excellent think probability favour country practitioner great deal visit foot. stick originally handsome knock hardly imagine town practitioner carry . thick iron ferrule wear evident great walk . perfectly sound say holmes. friend cch. guess hunt local hunt mem'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_chapters[0][:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['mr sherlock holmes mr sherlock holmes usually late morning save infrequent occasion night seat breakfast table',\n 'stand hearth rug pick stick visitor leave night',\n 'fine thick piece wood bulbous head sort know penang lawyer',\n 'head broad silver band nearly inch',\n 'james mortimer mrcs',\n 'friend cch',\n 'engrave date 1884',\n 'stick old fashion family practitioner carry dignify solid reassure',\n 'watson holmes sit give sign occupation',\n 'know believe eye head']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_chapter_sentences[0][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'(((0;;;0))) mr sherlock holmes mr sherlock holmes usually late morning save infrequent occasion night seat breakfast table\\n(((0;;;1))) stand hearth rug pick stick visitor leave night\\n(((0;;;2))) fine thick piece wood bulbous head sort know penang lawyer\\n(((0;;;3))) head broad silver band nearly inch\\n(((0;;;4))) james mortimer mrcs\\n(((0;;;5))) friend cch\\n(((0;;;6))) engrave date 1884\\n(((0;;;7))) stick old fashion family practitioner carry dignify solid reassure\\n(((0;;;8))) watson holmes sit give sign occupation\\n(((0;;;9))) know believe eye head\\n(((0;;;10))) polish silver plate coffee pot say\\n(((0;;;11))) tell watson visitor stick unfortunate miss notion errand accidental souvenir importance\\n(((0;;;12))) let hear reconstruct man examination\\n(((0;;;13))) think say follow far method companion dr mortimer successful elderly medical man esteem know mark appreciation\\n(((0;;;14))) good say holmes\\n(((0;;;15))) excellent think probability favour country practitioner great deal visit foot\\n(((0;;;'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked_text[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "def get_chapter_sentence(marked_text, word):\n",
    "    pattern = rf'^\\(\\(\\((\\d+;;;\\d+)\\)\\)\\).*?\\b({word})\\b.*?$'\n",
    "    matches = re.finditer(pattern, marked_text, re.MULTILINE | re.IGNORECASE)\n",
    "    return list(matches)\n",
    "\n",
    "matches = get_chapter_sentence(marked_text, 'sherlock')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "[<re.Match object; span=(0, 122), match='(((0;;;0))) mr sherlock holmes mr sherlock holmes>,\n <re.Match object; span=(3652, 3746), match='(((0;;;59))) laugh incredulously sherlock holmes >,\n <re.Match object; span=(5358, 5506), match='(((0;;;86))) dr james mortimer man science ask sh>,\n <re.Match object; span=(6549, 6614), match='(((0;;;107))) presume mr sherlock holmes address >,\n <re.Match object; span=(6972, 7028), match='(((0;;;114))) sherlock holmes wave strange visito>,\n <re.Match object; span=(13262, 13366), match='(((1;;;67))) dr mortimer finish read singular nar>,\n <re.Match object; span=(17163, 17257), match='(((1;;;112))) thank say sherlock holmes call atte>,\n <re.Match object; span=(21976, 22039), match='(((2;;;33))) sherlock holmes strike hand knee imp>,\n <re.Match object; span=(30584, 30683), match='(((3;;;5))) yes say strange thing mr sherlock hol>,\n <re.Match object; span=(31966, 32006), match='(((3;;;29))) promise say sherlock holmes'>,\n <re.Match object; span=(36858, 36922), match='(((3;;;106))) singularly useless thing steal say >,\n <re.Match object; span=(43137, 43213), match='(((4;;;0))) break thread sherlock holmes remarkab>,\n <re.Match object; span=(50088, 50146), match='(((4;;;133))) difficulty vanish easily say sherlo>,\n <re.Match object; span=(51923, 51957), match='(((4;;;168))) sherlock holmes note'>,\n <re.Match object; span=(52418, 52469), match='(((4;;;179))) mention say cabman mr sherlock holm>,\n <re.Match object; span=(52728, 52775), match='(((4;;;186))) sherlock holmes yes sir gentleman'>,\n <re.Match object; span=(53460, 53533), match='(((4;;;201))) leave turn round say interest know >,\n <re.Match object; span=(53587, 53648), match='(((4;;;204))) describe mr sherlock holmes cabman >,\n <re.Match object; span=(54697, 54769), match='(((5;;;1))) mr sherlock holmes drive station give>,\n <re.Match object; span=(68493, 68535), match='(((6;;;27))) answer report sherlock holmes'>,\n <re.Match object; span=(71576, 71701), match='(((6;;;71))) mr sherlock holmes word take away br>,\n <re.Match object; span=(71842, 71921), match='(((6;;;75))) follow mr sherlock holmes interest m>,\n <re.Match object; span=(82731, 82839), match='(((7;;;0))) report dr watson point onward follow >,\n <re.Match object; span=(110428, 110519), match='(((9;;;0))) extract diary dr watson far able quot>,\n <re.Match object; span=(119123, 119162), match='(((9;;;169))) live year sherlock holmes'>,\n <re.Match object; span=(148597, 148658), match='(((11;;;214))) think mr sherlock holmes friend bo>,\n <re.Match object; span=(151476, 151575), match='(((12;;;0))) fix net sir henry please surprise sh>,\n <re.Match object; span=(159165, 159269), match='(((12;;;147))) laura lyons office sherlock holmes>,\n <re.Match object; span=(160175, 160220), match='(((12;;;169))) sherlock holmes shrug shoulder'>,\n <re.Match object; span=(161073, 161130), match='(((12;;;187))) entirely believe madam say sherloc>,\n <re.Match object; span=(161824, 161881), match='(((12;;;200))) think fortunate escape say sherloc>,\n <re.Match object; span=(162989, 163111), match='(((13;;;0))) hound baskervilles sherlock holmes d>]"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def porters_alg(text):\n",
    "    # https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/#:~:text=The%20Porter%20Stemming%20algorithm%20(or,of%20Information%20Retrieval%20(IR).\n",
    "    # https://tartarus.org/martin/PorterStemmer/\n",
    "    m = re.match(r'(\\w+?)(?=ly|es|(?<!s)s|y)', text)\n",
    "    print(m.groups())\n",
    "porters_alg('caresses days cates')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_chapters[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_chapters[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(filter(lambda c: len(c) > 50, raw_chapters))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.regex_utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "textloc = \"books/a-study-in-scarlet.txt\"\n",
    "stopwordloc = 'project1\\stopwords.txt'\n",
    "text = GetTextFromFile(textloc)\n",
    "stopwords = GetStopWords(stopwordloc)\n",
    "clean_text = GetCleanText(text, stopwords)\n",
    "wordlist = GetUniqueWordList(clean_text)\n",
    "chapters = GetChapterTextList(text, stopwords)\n",
    "chapters_wordlist = GetChapterWordList(chapters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "textloc = \"books/a-study-in-scarlet.txt\"\n",
    "stopwordloc = 'project1\\stopwords.txt'\n",
    "text = GetTextFromFile(textloc)\n",
    "stopwords = GetStopWords(stopwordloc)\n",
    "clean_text = GetCleanText(text, stopwords)\n",
    "wordlist = GetUniqueWordList(clean_text)\n",
    "chapters = GetChapterTextList(text, stopwords)\n",
    "chapters_wordlist = GetChapterWordList(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'w',encoding='utf-8') as f:\n",
    "    for w in nlp.Defaults.stop_words:\n",
    "        f.write(w + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b964ac9b30b36bbea2ffa1ada2bc35f19c0e363ef9f5256b2512afd4c34d440"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}