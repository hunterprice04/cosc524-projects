{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikiann (C:/Users/zimin/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2cf55b229d45c19fe6db8f0e5d53af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'O', '1': 'B-MIS', '2': 'I-MIS', '3': 'B-PER', '4': 'I-PER', '5': 'B-ORG', '6': 'I-ORG', '7': 'B-LOC', '8': 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"wikiann\", 'en')\n",
    "dataset_feature = raw_dataset[\"train\"].features\n",
    "ner_labels = dataset_feature[\"ner_tags\"].feature.names\n",
    "id2label = {'0': 'O', '1': 'B-MIS', '2': 'I-MIS', '3': 'B-PER', '4': 'I-PER', '5': 'B-ORG', '6': 'I-ORG', '7': 'B-LOC', '8': 'I-LOC'}\n",
    "label2id = {value: key for key, value in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_and_tokens(word_ids, labels):\n",
    "    updated_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            updated_labels.append(-100 if word_id is None else labels[word_id])   \n",
    "        elif word_id is None:\n",
    "            updated_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label+=1\n",
    "            updated_labels.append(label)\n",
    "    return updated_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/zimin/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e\\cache-eab2f4ebde86760d.arrow\n",
      "Loading cached processed dataset at C:/Users/zimin/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e\\cache-cf2c59578e7dae45.arrow\n",
      "Loading cached processed dataset at C:/Users/zimin/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e\\cache-0ff7e2b26b644afd.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(dataset):\n",
    "    tokenized_data = tokenizer(dataset[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = dataset[\"ner_tags\"]\n",
    "    updated_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        updated_labels.append(align_labels_and_tokens(tokenized_data.word_ids(i), labels))\n",
    "    tokenized_data[\"labels\"] = updated_labels\n",
    "    return tokenized_data\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "batch_size = 16\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched = True,\n",
    "    remove_columns = raw_dataset[\"train\"].column_names\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimin\\AppData\\Local\\Temp\\ipykernel_17964\\1449754308.py:24: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels= 9,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id\n",
    ")\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    model_checkpoint,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p \n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ner_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": metrics[\"overall_precision\"],\n",
    "        \"recall\": metrics[\"overall_recall\"],\n",
    "        \"f1\": metrics[\"overall_f1\"],\n",
    "        \"accuracy\": metrics[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zimin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3750\n",
      "  Number of trainable parameters = 107726601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebaa09aa808479190d6f0d283e1fcf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-500\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.442, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-1000\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3397, 'learning_rate': 7.333333333333333e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-1000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3db680b09e4f589d18684e345c5f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30720797181129456, 'eval_precision': 0.7701528722472893, 'eval_recall': 0.8084264102926623, 'eval_f1': 0.7888256595964821, 'eval_accuracy': 0.9118443776244619, 'eval_runtime': 201.8974, 'eval_samples_per_second': 49.53, 'eval_steps_per_second': 3.096, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-1500\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2575, 'learning_rate': 6e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-2000\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.209, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-2500\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1957, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-2500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29c36411e404d7d898e35884043165a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2730152904987335, 'eval_precision': 0.7843137254901961, 'eval_recall': 0.8228474480418493, 'eval_f1': 0.8031186393900716, 'eval_accuracy': 0.9174549184279966, 'eval_runtime': 200.2464, 'eval_samples_per_second': 49.938, 'eval_steps_per_second': 3.121, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-3000\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.105, 'learning_rate': 2e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to dslim/bert-base-NER\\checkpoint-3500\n",
      "Configuration saved in dslim/bert-base-NER\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1075, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in dslim/bert-base-NER\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in dslim/bert-base-NER\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in dslim/bert-base-NER\\checkpoint-3500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e14e36ad7544fb4ac9e6a71278bdae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to fine-tune-1.model\n",
      "Configuration saved in fine-tune-1.model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31454959511756897, 'eval_precision': 0.8040067911714771, 'eval_recall': 0.8369150289834583, 'eval_f1': 0.8201309272280143, 'eval_accuracy': 0.921987183420852, 'eval_runtime': 209.838, 'eval_samples_per_second': 47.656, 'eval_steps_per_second': 2.978, 'epoch': 3.0}\n",
      "{'train_runtime': 6498.3994, 'train_samples_per_second': 9.233, 'train_steps_per_second': 0.577, 'train_loss': 0.2275054712931315, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in fine-tune-1.model\\pytorch_model.bin\n",
      "tokenizer config file saved in fine-tune-1.model\\tokenizer_config.json\n",
      "Special tokens file saved in fine-tune-1.model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset =  tokenized_dataset[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train() \n",
    "trainer.save_model('fine-tune-1.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b964ac9b30b36bbea2ffa1ada2bc35f19c0e363ef9f5256b2512afd4c34d440"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
