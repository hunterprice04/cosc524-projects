{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Part 3 - Auto-Summarization\n",
    "For the last part, you will use the sections of your book of choice that are\n",
    "\n",
    "    a) the most descriptive of the crime and\n",
    "    b) the most descriptive of the resolution of the crime (e.g., description and uncovering of the perpetrator).\n",
    "\n",
    "You will use the sections that are at the minimum 256 tokens long, and you will test the summarization using T5 model. You will then assess and analyze the presence of the key and relevant facts in the summarized material.\n",
    "\n",
    "For the extra credit: Create your own manually summarized content and then use [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) score to showcase the performance of the auto-summarized content vs. manually produced."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-11-19 15:56:57.018028: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-19 15:56:57.079839: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-19 15:56:57.096086: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-19 15:56:57.377437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-19 15:56:57.377466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-19 15:56:57.377469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-19 15:56:57.805743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:56:57.818886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:56:57.818976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# io\n",
    "import os\n",
    "import re\n",
    "\n",
    "# sentence tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "# huggingface\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Globals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "INPUT_DIR = 'part3-input'\n",
    "OUTPUT_DIR = 'part3-output'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Summarization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Setup model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp/.conda/envs/huggingface/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "2022-11-19 15:57:02.005309: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-19 15:57:02.006041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:57:02.006136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:57:02.006181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:57:02.295914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:57:02.296013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:57:02.296065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-19 15:57:02.296114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7373 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-11-19 15:57:03.017308: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "pipe = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Load in text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text filenames: ['part3-input/inp1.txt']\n",
      "reference text filenames: ['part3-input/ref1.txt']\n"
     ]
    }
   ],
   "source": [
    "# get number of samples\n",
    "sample_fn = os.listdir(INPUT_DIR)\n",
    "n_samples = len(sample_fn)//2\n",
    "\n",
    "# get filenames\n",
    "inp_fn = [os.path.join(INPUT_DIR, f'inp{i}.txt') for i in range(1,n_samples+1)]\n",
    "ref_fn = [os.path.join(INPUT_DIR, f'ref{i}.txt') for i in range(1,n_samples+1)]\n",
    "print('input text filenames:', inp_fn)\n",
    "print('reference text filenames:', ref_fn)\n",
    "\n",
    "# load in texts\n",
    "def clean_context(filename):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    text = re.sub(\"\\n\", r' ', text)\n",
    "    text = re.sub(r\"\\s{2,}\", r' ', text)\n",
    "    text = re.sub(r\"“|”\", r'\"', text)\n",
    "    text = re.sub(r\"‘|’\", r\"'\", text)\n",
    "    text = re.sub(r\"_\", r'', text, re.ASCII)\n",
    "    text = re.sub(r\"\\s{2,}\", r' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "inp_text = [clean_context(fn) for fn in inp_fn]\n",
    "ref_text = [clean_context(fn) for fn in ref_fn]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----\n",
    "# Summarize text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Automate Summarization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def prepare(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentences = [str(s) for s in sentences]\n",
    "\n",
    "    length = 0\n",
    "    chunk = \"\"\n",
    "    chunks = []\n",
    "    count = -1\n",
    "\n",
    "    for sentence in sentences:\n",
    "        count += 1\n",
    "        combined_length = len(tokenizer.tokenize(sentence)) + length\n",
    "\n",
    "        if combined_length <= tokenizer.max_len_single_sentence:\n",
    "            chunk += sentence + \" \"\n",
    "            length = combined_length\n",
    "            if count == len(sentences) - 1:\n",
    "                chunks.append(chunk.strip())\n",
    "\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = \"\"\n",
    "            chunk += sentence + \" \"\n",
    "            length = len(tokenizer.tokenize(sentence))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_output(chunks):\n",
    "    inputs = [tokenizer(chunk, return_tensors='tf') for chunk in chunks]\n",
    "    outputs = []\n",
    "    for input in inputs:\n",
    "        output = model.generate(**input)\n",
    "        outputs.append(tokenizer.decode(*output, skip_special_tokens=True))\n",
    "    out_sent = []\n",
    "    for output in outputs:\n",
    "        out_sent += sent_tokenize(output)\n",
    "    output = \"\\n\".join(out_sent)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    chunks = prepare(text)\n",
    "    output = get_output(chunks)\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Summarize all the input texts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp/.conda/envs/huggingface/lib/python3.8/site-packages/transformers/generation_tf_utils.py:1690: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 200 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\"The old gentleman was not to be decoyed.\n",
      "He could not be deceived.\n",
      "Stapleton waited.\n",
      "He waited and waited, but he waited no longer.\n",
      "\"Stapleton was determined.\n",
      "He was determined to kill Sir Charles.\n",
      "\"He waited for the old gentleman to come.\n",
      "He had hoped that his wife might lure him to his ruin, but she refused.\n",
      "\"But he was not. \"\n",
      "he.\n",
      "\"It was not long before He \"\n",
      ".\n",
      "It is a case which has remained unsolved for many years.\n",
      "It has...\n",
      "It was a dreadful sight to see that huge black creature bounding after its victim.\n",
      "It must have been a terrible sight indeed to see.\n",
      "In that gloomy tunnel it must indeed have been awful to see hound left.............. creature\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for inp in inp_text:\n",
    "    predictions.append(predict(inp))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Evaluation with Rouge Score(Extra Credit)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Setup Rouge Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def evaluate(predictions, references):\n",
    "    global rouge\n",
    "    results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references\n",
    "    )\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
