{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "# Part 3 - Auto-Summarization\n",
    "For the last part, you will use the sections of your book of choice that are\n",
    "\n",
    "    a) the most descriptive of the crime and\n",
    "    b) the most descriptive of the resolution of the crime (e.g., description and uncovering of the perpetrator).\n",
    "\n",
    "You will use the sections that are at the minimum 256 tokens long, and you will test the summarization using T5 model. You will then assess and analyze the presence of the key and relevant facts in the summarized material.\n",
    "\n",
    "For the extra credit: Create your own manually summarized content and then use [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) score to showcase the performance of the auto-summarized content vs. manually produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-12-03 19:19:49.496070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 19:19:49.570408: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-03 19:19:49.593597: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-03 19:19:50.001196: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-03 19:19:50.001232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-03 19:19:50.001234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-03 19:19:50.798282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:50.860966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:50.861054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# io\n",
    "import os\n",
    "import re\n",
    "\n",
    "# sentence tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "# huggingface\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = 'part3-text'\n",
    "OUTPUT_DIR = 'part3-text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 19:19:53.411694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 19:19:53.412620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:53.412744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:53.412794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:53.786181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:53.793286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:53.793345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 19:19:53.793399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7079 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-12-03 19:19:55.023539: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "/home/hp/.conda/envs/huggingface/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-large\", device=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "### Load in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text filenames: ['part3-text/inp1.txt', 'part3-text/inp2.txt', 'part3-text/inp3.txt', 'part3-text/inp4.txt']\n",
      "reference text filenames: ['part3-text/ref1.txt', 'part3-text/ref2.txt', 'part3-text/ref3.txt', 'part3-text/ref4.txt']\n"
     ]
    }
   ],
   "source": [
    "# get number of samples\n",
    "sample_fn = os.listdir(INPUT_DIR)\n",
    "sample_fn = list(filter(lambda x: 'inp' in x.lower(), sample_fn))\n",
    "n_samples = len(sample_fn)\n",
    "\n",
    "# get filenames\n",
    "inp_fn = [os.path.join(INPUT_DIR, f'inp{i}.txt') for i in range(1,n_samples+1)]\n",
    "ref_fn = [os.path.join(INPUT_DIR, f'ref{i}.txt') for i in range(1,n_samples+1)]\n",
    "print('input text filenames:', inp_fn)\n",
    "print('reference text filenames:', ref_fn)\n",
    "\n",
    "# load in texts\n",
    "def clean_context(filename):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    text = re.sub(\"\\n\", r' ', text)\n",
    "    text = re.sub(r\"\\s{2,}\", r' ', text)\n",
    "    text = re.sub(r\"“|”\", r'\"', text)\n",
    "    text = re.sub(r\"‘|’\", r\"'\", text)\n",
    "    text = re.sub(r\"_\", r'', text, re.ASCII)\n",
    "    text = re.sub(r\"\\s{2,}\", r' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "inp_text = [clean_context(fn) for fn in inp_fn]\n",
    "ref_text = [\"\\n\".join(sent_tokenize(clean_context(fn))) for fn in ref_fn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The circumstances connected with the death of Sir Charles cannot be said to have been entirely cleared up by the inquest, but at least enough has been done to dispose of those rumours to which local superstition has given rise. There is no reason whatever to suspect foul play, or to imagine that death could be from any but natural causes. Sir Charles was a widower, and a man who may be said to have been in some ways of an eccentric habit of mind. In spite of his considerable wealth he was simple in his personal tastes, and his indoor servants at Baskerville Hall consisted of a married couple named Barrymore, the husband acting as butler and the wife as housekeeper. Their evidence, corroborated by that of several friends, tends to show that Sir Charles\\'s health has for some time been impaired, and points especially to some affection of the heart, manifesting itself in changes of colour, breathlessness, and acute attacks of nervous depression. Dr. James Mortimer, the friend and medical attendant of the deceased, has given evidence to the same effect.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----\n",
    "# Summarize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "### Summarize all the input texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing Input 0\n",
      "Summarizing Input 1\n",
      "Summarizing Input 2\n",
      "Summarizing Input 3\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i, inp in enumerate(inp_text):\n",
    "    print(f'Summarizing Input {i}')\n",
    "    predictions.append(pipe(inp))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_pred(preds):\n",
    "    out = []\n",
    "    for p in preds:\n",
    "        for x in p:\n",
    "            out.append('\\n'.join(sent_tokenize(x['summary_text'])))\n",
    "    return out\n",
    "predictions = clean_pred(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"sir Charles Baskerville was in the habit every night before going to bed\" \"the evidence of the Barrymores shows that this had been his custom,\" \"the coroner\\'s jury returned a verdict in accordance with the medical evidence\" \"it is obviously of the utmost importance that Sir Charles\\'s heir should settle at the Hall\"',\n",
       " '\"he found a way out of his difficulties through the chance that sir Charles made him minister of his charity in the case of unfortunate woman, Mrs. Laura Lyons\" by representing himself as a single man he acquired complete influence over her .\\nhe then put pressure upon Mrs. Lyons to write this letter, imploring the old man to give her an interview on the evening before his departure .',\n",
       " '\"the other words were all simple and might be found in any issue, but \\'moor\\' would be less common\" \"have you read anything else in this message, Mr.\\nHolmes?\"\\nhe asks .\\n\"the address, you observe, is printed in rough characters, but the times is a paper which is seldom found in anyone but those of the highly educated\"',\n",
       " '\"there is no reason whatsoever to suspect foul play,\" says sir charles barry .\\nbarry: \"the circumstances connected with the death of sir Charles cannot be said to have been entirely cleared up by the inquest\"']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(predictions):\n",
    "    with open(os.path.join(OUTPUT_DIR, f'out{i+1}.txt'), 'w') as f:\n",
    "        f.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "# Evaluation with Rouge Score(Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "### Setup Rouge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def eval_rouge(predictions, references):\n",
    "    global rouge\n",
    "    results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references\n",
    "    )\n",
    "    return results\n",
    "res = eval_rouge(predictions, ref_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.30477971889501143,\n",
       " 'rouge2': 0.08313524369280692,\n",
       " 'rougeL': 0.172717101887657,\n",
       " 'rougeLsum': 0.2431638779022941}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Sir Charles Baskerville did not return after his nightly walk. Barrymore noticed and followed Sir Charles' footsteps. It seemed Sir Charles stood by the gate to the moor then walked down the alley. Barrymore then found Sir Charles' body at the end of the alley. The body did not show any signs of struggle. Interestingly, the victim's face showed incredible facial distortion.\",\n",
       " 'Stapleton convinced Mrs. Laura Lyons to write a letter to Sir Charles telling him to give her an interview. He then painted his hound and brought it to the gate. The hound jumped over the gate and chased the baronet until he fell dead at the end of the alley from terror.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b964ac9b30b36bbea2ffa1ada2bc35f19c0e363ef9f5256b2512afd4c34d440"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
