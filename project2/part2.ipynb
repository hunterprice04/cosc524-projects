{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Project 2 - Part 2\n",
    "For Part 2, attempt to fine-tune the question-answering with your questions and answers intended to offer greater resolution than the answers in Part 1.\n",
    "    To do this, you will need to generate your own custom SQuAD-compatible QA dataset using the sections of your choice, the questions, and the answers which you will add to the squad dataset downloadable from Hugging Face, [which is described in this article](https://huggingface.co/transformers/v3.2.0/custom_datasets.html).  Note: custom training can take a very long time on Google Colab, so be prepared for that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# check tensorflow device\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Read in dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    print(len(squad_dict['data']))\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for i, group in enumerate(squad_dict['data']):\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "        if i > len(squad_dict['data'])/2:\n",
    "            break\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two â€“ fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 18:01:24.107037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-24 18:01:24.107929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 18:01:24.108027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 18:01:24.108072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 18:01:24.376442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 18:01:24.376538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 18:01:24.376584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 18:01:24.376630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7210 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 18:01:43.588482: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForQuestionAnswering: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Keras will expect a tuple when dealing with labels\n",
    "train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['_SCALAR_UPRANKING_ON',\n '_TF_MODULE_IGNORED_PROPERTIES',\n '__call__',\n '__class__',\n '__copy__',\n '__deepcopy__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_activity_regularizer',\n '_add_trackable',\n '_add_trackable_child',\n '_add_variable_with_custom_getter',\n '_assert_compile_was_called',\n '_assert_weights_created',\n '_auto_class',\n '_auto_track_sub_layers',\n '_autocast',\n '_autographed_call',\n '_base_model_initialized',\n '_build_input_shape',\n '_call_spec',\n '_callable_losses',\n '_captured_weight_regularizer',\n '_cast_single_input',\n '_check_call_args',\n '_check_sample_weight_warning',\n '_checkpoint',\n '_checkpoint_dependencies',\n '_clear_losses',\n '_cluster_coordinator',\n '_compile_was_called',\n '_compiled_trainable_state',\n '_compute_dtype',\n '_compute_dtype_object',\n '_compute_output_and_mask_jointly',\n '_configure_steps_per_execution',\n '_create_repo',\n '_dedup_weights',\n '_deferred_dependencies',\n '_delete_tracking',\n '_deserialization_dependencies',\n '_deserialize_from_proto',\n '_distribution_strategy',\n '_dtype',\n '_dtype_policy',\n '_dynamic',\n '_eager_losses',\n '_expand_inputs_for_generation',\n '_expand_to_num_beams',\n '_expects_mask_arg',\n '_expects_training_arg',\n '_export_to_saved_model_graph',\n '_flatten',\n '_flatten_layers',\n '_flatten_modules',\n '_from_config',\n '_functional_construction_call',\n '_gather_children_attribute',\n '_gather_saveables_for_checkpoint',\n '_generate',\n '_generate_beam_search',\n '_get_callback_model',\n '_get_cell_name',\n '_get_compile_args',\n '_get_decoder_start_token_id',\n '_get_existing_metric',\n '_get_files_timestamps',\n '_get_input_masks',\n '_get_logits_processor',\n '_get_logits_warper',\n '_get_node_attribute_at_index',\n '_get_optimizer',\n '_get_resized_embeddings',\n '_get_resized_lm_head_bias',\n '_get_resized_lm_head_decoder',\n '_get_save_spec',\n '_get_trainable_state',\n '_get_unnested_name_scope',\n '_get_word_embedding_weight',\n '_handle_activity_regularization',\n '_handle_deferred_dependencies',\n '_handle_weight_regularization',\n '_in_multi_worker_mode',\n '_inbound_nodes',\n '_inbound_nodes_value',\n '_infer_output_signature',\n '_init_batch_counters',\n '_init_call_fn_args',\n '_init_set_name',\n '_initial_weights',\n '_input_spec',\n '_instrument_layer_creation',\n '_instrumented_keras_api',\n '_instrumented_keras_layer_class',\n '_instrumented_keras_model_class',\n '_is_compiled',\n '_is_graph_network',\n '_is_layer',\n '_is_model_for_instrumentation',\n '_jit_compile',\n '_keras_api_names',\n '_keras_api_names_v1',\n '_keras_tensor_symbolic_call',\n '_keys_to_ignore_on_load_missing',\n '_keys_to_ignore_on_load_unexpected',\n '_label_to_output_map',\n '_layout_map',\n '_lookup_dependency',\n '_losses',\n '_map_resources',\n '_maybe_build',\n '_maybe_cast_inputs',\n '_maybe_create_attribute',\n '_maybe_initialize_trackable',\n '_maybe_load_initial_counters_from_ckpt',\n '_maybe_load_initial_step_from_ckpt',\n '_metrics',\n '_metrics_lock',\n '_must_restore_from_config',\n '_name',\n '_name_based_attribute_restore',\n '_name_based_restores',\n '_name_scope',\n '_name_scope_on_declaration',\n '_no_dependency',\n '_non_trainable_weights',\n '_obj_reference_counts',\n '_obj_reference_counts_dict',\n '_object_identifier',\n '_outbound_nodes',\n '_outbound_nodes_value',\n '_predict_counter',\n '_preload_simple_restoration',\n '_prepare_attention_mask_for_generation',\n '_prepare_decoder_input_ids_for_generation',\n '_prepare_encoder_decoder_kwargs_for_generation',\n '_prepare_model_inputs',\n '_preserve_input_structure_in_config',\n '_reorder_cache',\n '_requires_load_weight_prefix',\n '_reset_compile_cache',\n '_resize_token_embeddings',\n '_restore_from_tensors',\n '_run_eagerly',\n '_save_checkpoint',\n '_save_new',\n '_saved_model_arg_spec',\n '_saved_model_inputs_spec',\n '_seed_generator',\n '_self_name_based_restores',\n '_self_saveable_object_factories',\n '_self_setattr_tracking',\n '_self_tracked_trackables',\n '_self_unconditional_checkpoint_dependencies',\n '_self_unconditional_deferred_dependencies',\n '_self_unconditional_dependency_names',\n '_self_update_uid',\n '_serialize_to_proto',\n '_serialize_to_tensors',\n '_set_connectivity_metadata',\n '_set_dtype_policy',\n '_set_inputs',\n '_set_mask_keras_history_checked',\n '_set_mask_metadata',\n '_set_save_spec',\n '_set_trainable_state',\n '_set_training_mode',\n '_setattr_tracking',\n '_should_cast_single_input',\n '_should_compute_mask',\n '_should_eval',\n '_stateful',\n '_steps_per_execution',\n '_supports_masking',\n '_test_counter',\n '_tf_api_names',\n '_tf_api_names_v1',\n '_thread_local',\n '_track_trackable',\n '_trackable_children',\n '_trackable_saved_model_saver',\n '_tracking_metadata',\n '_train_counter',\n '_trainable',\n '_trainable_weights',\n '_training_state',\n '_unconditional_checkpoint_dependencies',\n '_unconditional_dependency_names',\n '_undeduplicated_weights',\n '_update_model_kwargs_for_generation',\n '_update_model_kwargs_for_xla_generation',\n '_update_uid',\n '_updated_config',\n '_updates',\n '_upload_modified_files',\n '_use_cache',\n '_use_input_spec_as_call_signature',\n '_using_dummy_loss',\n '_v2_get_resized_embeddings',\n '_v2_get_resized_lm_head_bias',\n '_v2_resize_token_embeddings',\n '_v2_resized_token_embeddings',\n '_validate_compile',\n '_validate_model_class',\n '_validate_model_kwargs',\n '_validate_target_and_loss',\n 'activity_regularizer',\n 'add_loss',\n 'add_metric',\n 'add_update',\n 'add_variable',\n 'add_weight',\n 'adjust_logits_during_generation',\n 'base_model_prefix',\n 'beam_search',\n 'build',\n 'built',\n 'call',\n 'compile',\n 'compiled_loss',\n 'compiled_metrics',\n 'compute_dtype',\n 'compute_loss',\n 'compute_mask',\n 'compute_metrics',\n 'compute_output_shape',\n 'compute_output_signature',\n 'config',\n 'config_class',\n 'count_params',\n 'create_model_card',\n 'distilbert',\n 'distribute_strategy',\n 'dropout',\n 'dtype',\n 'dtype_policy',\n 'dummy_inputs',\n 'dynamic',\n 'evaluate',\n 'evaluate_generator',\n 'finalize_state',\n 'fit',\n 'fit_generator',\n 'framework',\n 'from_config',\n 'from_pretrained',\n 'generate',\n 'get_bias',\n 'get_config',\n 'get_input_at',\n 'get_input_embeddings',\n 'get_input_mask_at',\n 'get_input_shape_at',\n 'get_label_to_output_name_mapping',\n 'get_layer',\n 'get_lm_head',\n 'get_output_at',\n 'get_output_embeddings',\n 'get_output_layer_with_bias',\n 'get_output_mask_at',\n 'get_output_shape_at',\n 'get_prefix_bias_name',\n 'get_weight_paths',\n 'get_weights',\n 'greedy_search',\n 'hf_compute_loss',\n 'history',\n 'inbound_nodes',\n 'input',\n 'input_mask',\n 'input_names',\n 'input_shape',\n 'input_spec',\n 'inputs',\n 'layers',\n 'load_repo_checkpoint',\n 'load_weights',\n 'losses',\n 'main_input_name',\n 'make_predict_function',\n 'make_test_function',\n 'make_train_function',\n 'metrics',\n 'metrics_names',\n 'name',\n 'name_or_path',\n 'name_scope',\n 'non_trainable_variables',\n 'non_trainable_weights',\n 'num_parameters',\n 'optimizer',\n 'outbound_nodes',\n 'output',\n 'output_mask',\n 'output_names',\n 'output_shape',\n 'outputs',\n 'predict',\n 'predict_function',\n 'predict_generator',\n 'predict_on_batch',\n 'predict_step',\n 'prepare_tf_dataset',\n 'prune_heads',\n 'push_to_hub',\n 'qa_outputs',\n 'register_for_auto_class',\n 'reset_metrics',\n 'reset_states',\n 'resize_token_embeddings',\n 'run_eagerly',\n 'sample',\n 'save',\n 'save_pretrained',\n 'save_spec',\n 'save_weights',\n 'seed_generator',\n 'serving',\n 'serving_output',\n 'set_bias',\n 'set_input_embeddings',\n 'set_output_embeddings',\n 'set_weights',\n 'state_updates',\n 'stateful',\n 'stop_training',\n 'submodules',\n 'summary',\n 'supports_masking',\n 'supports_xla_generation',\n 'test_function',\n 'test_on_batch',\n 'test_step',\n 'to_json',\n 'to_yaml',\n 'train_function',\n 'train_on_batch',\n 'train_step',\n 'train_tf_function',\n 'trainable',\n 'trainable_variables',\n 'trainable_weights',\n 'updates',\n 'variable_dtype',\n 'variables',\n 'weights',\n 'with_name_scope']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n",
    "# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n",
    "# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.distilbert.return_dict = False # if using ðŸ¤— Transformers >3.02, make sure outputs are tuples\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5676/5676 [==============================] - 813s 143ms/step - loss: nan - output_1_loss: nan - output_2_loss: nan\n",
      "Epoch 2/3\n",
      "5640/5676 [============================>.] - ETA: 5s - loss: nan - output_1_loss: nan - output_2_loss: nan"
     ]
    }
   ],
   "source": [
    "# finetune the model with the dataset then save the weights to the \"model-weights\" directory\n",
    "os.makedirs('model-weights', exist_ok=True)\n",
    "model.fit(train_dataset.shuffle(1000).batch(8), epochs=3, batch_size=16, callbacks=[tensorboard_callback])\n",
    "model.save_pretrained('model-weights')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "#  Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the finetuned weights\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained('model-weights')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Free up some memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# del tokenizer, nlp, model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
